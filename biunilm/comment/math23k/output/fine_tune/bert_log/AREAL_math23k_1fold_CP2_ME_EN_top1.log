[Wed, 01 Sep 2021 08:53:04] INFO [main: run_main_batch.py, 52] pid:48265, epoch:80, args:Namespace(Fold=1, add_analogy_embedding=True, add_copynet=True, add_memory_module=True, add_memory_valid=True, add_num_equ_ids=True, add_tokens_a_padding=True, always_truncate_tail=True, amp=False, attention_probs_dropout_prob=0.1, batch_size=8, beam_size=1, bert_model='bert-base-chinese', config_path=None, dataset='math23k', do_eval=False, do_l2r_training=False, do_lower_case=False, do_train=True, easy_to_hard=True, eval_batch_size=64, ffn_type=0, finetune_decay=False, forbid_duplicate_ngrams=False, forbid_ignore_word='.', fp16=False, fp32_embedding=False, from_scratch=False, gradient_accumulation_steps=1, has_sentence_oracle=False, hidden_dropout_prob=0.1, is_debug=True, is_delete_early_model=False, is_equ_norm=True, is_single_char=False, is_train=False, label_smoothing=0.1, learning_rate=2e-05, length_penalty=0, local_rank=-1, log_dir='./biunilm/comment/math23k/output/fine_tune/bert_log', loss_scale=0, mask_prob=0.15, mask_prob_eos=0, mask_source_words=False, mask_whole_word=True, max_analogy_len=512, max_len_a=192, max_len_b=64, max_position_embeddings=512, max_pred=64, max_seq_length=256, max_tgt_length=64, memory_train_file='./preprocess/sim_result/sim_question_by_w2v_train_math23k_equNorm_1fold_top5.pkl', memory_valid_file='./preprocess/sim_result/sim_question_by_w2v_valid_math23k_equNorm_1fold_top5.pkl', min_len=None, mode='s2s', model_recover_path='./biunilm/comment/math23k/output/fine_tune/AREAL_math23k_1fold_CP2_ME_EN_top1/model.epoch.bin', need_score_traces=True, new_pos_ids=False, new_segment_ids=False, ngram_size=3, no_cuda=False, not_predict_token=None, num_equ_size=3, num_qkv=0, num_train_epochs=80, num_workers=0, optim_recover_path=None, output_dir='./biunilm/comment/math23k/output/fine_tune/AREAL_math23k_1fold_CP2_ME_EN_top1', pos_shift=True, pred_wo_memory_copy=False, relax_projection=False, repeat='', s2s_add_segment=False, s2s_share_segment=False, s2s_special_token=False, save_every_epoch=False, seed=42, seg_emb=False, skipgram_prb=0.0, skipgram_size=1, split='test', start_lr_decay_epoch=40, subset=0, tokenized_input=False, topk=1, train_batch_size=2, trunc_seg='a', used_bertAdam=False, used_lr_decay=True, warmup_proportion=0.1, weight_decay=0.01)
[Wed, 01 Sep 2021 08:53:05] INFO [from_pretrained: tokenization.py, 197] loading vocabulary file https://s3.amazonaws.com/models.huggingface.co/bert/bert-base-chinese-vocab.txt from cache at ..comment/tmp_folder/bert-cased-pretrained-cache/8a0c070123c1f794c42a29c6904beb7c1b8715741e235bee04aca2c7636fc83f.9b42061518a39ca00b8b52059fd2bede8daa613f8a8671500e518a8c29de8c00
[Wed, 01 Sep 2021 08:53:05] INFO [main_generation: decoder_seq2seq_mwp.py, 194] ./biunilm/comment/math23k/output/fine_tune/AREAL_math23k_1fold_CP2_ME_EN_top1/model.80.bin
[Wed, 01 Sep 2021 08:54:18] INFO [main: run_main_batch.py, 52] pid:48821, epoch:80, args:Namespace(Fold=1, add_analogy_embedding=True, add_copynet=True, add_memory_module=True, add_memory_valid=True, add_num_equ_ids=True, add_tokens_a_padding=True, always_truncate_tail=True, amp=False, attention_probs_dropout_prob=0.1, batch_size=8, beam_size=1, bert_model='bert-base-chinese', config_path=None, dataset='math23k', do_eval=False, do_l2r_training=False, do_lower_case=False, do_train=True, easy_to_hard=True, eval_batch_size=64, ffn_type=0, finetune_decay=False, forbid_duplicate_ngrams=False, forbid_ignore_word='.', fp16=False, fp32_embedding=False, from_scratch=False, gradient_accumulation_steps=1, has_sentence_oracle=False, hidden_dropout_prob=0.1, is_debug=True, is_delete_early_model=False, is_equ_norm=True, is_single_char=False, is_train=False, label_smoothing=0.1, learning_rate=2e-05, length_penalty=0, local_rank=-1, log_dir='biunilm/comment/math23k/output/fine_tune/bert_log', loss_scale=0, mask_prob=0.15, mask_prob_eos=0, mask_source_words=False, mask_whole_word=True, max_analogy_len=512, max_len_a=192, max_len_b=64, max_position_embeddings=512, max_pred=64, max_seq_length=256, max_tgt_length=64, memory_train_file='./preprocess/sim_result/sim_question_by_w2v_train_math23k_equNorm_1fold_top5.pkl', memory_valid_file='./preprocess/sim_result/sim_question_by_w2v_valid_math23k_equNorm_1fold_top5.pkl', min_len=None, mode='s2s', model_recover_path='biunilm/comment/math23k/output/fine_tune/AREAL_math23k_1fold_CP2_ME_EN_top1/model.epoch.bin', need_score_traces=True, new_pos_ids=False, new_segment_ids=False, ngram_size=3, no_cuda=False, not_predict_token=None, num_equ_size=3, num_qkv=0, num_train_epochs=80, num_workers=0, optim_recover_path=None, output_dir='biunilm/comment/math23k/output/fine_tune/AREAL_math23k_1fold_CP2_ME_EN_top1', pos_shift=True, pred_wo_memory_copy=False, relax_projection=False, repeat='', s2s_add_segment=False, s2s_share_segment=False, s2s_special_token=False, save_every_epoch=False, seed=42, seg_emb=False, skipgram_prb=0.0, skipgram_size=1, split='test', start_lr_decay_epoch=40, subset=0, tokenized_input=False, topk=1, train_batch_size=2, trunc_seg='a', used_bertAdam=False, used_lr_decay=True, warmup_proportion=0.1, weight_decay=0.01)
[Wed, 01 Sep 2021 08:54:19] INFO [from_pretrained: tokenization.py, 197] loading vocabulary file https://s3.amazonaws.com/models.huggingface.co/bert/bert-base-chinese-vocab.txt from cache at ..comment/tmp_folder/bert-cased-pretrained-cache/8a0c070123c1f794c42a29c6904beb7c1b8715741e235bee04aca2c7636fc83f.9b42061518a39ca00b8b52059fd2bede8daa613f8a8671500e518a8c29de8c00
[Wed, 01 Sep 2021 08:54:19] INFO [main_generation: decoder_seq2seq_mwp.py, 194] biunilm/comment/math23k/output/fine_tune/AREAL_math23k_1fold_CP2_ME_EN_top1/model.80.bin
