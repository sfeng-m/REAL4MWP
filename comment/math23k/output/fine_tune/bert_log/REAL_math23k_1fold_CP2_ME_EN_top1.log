[Mon, 06 Sep 2021 07:00:54] INFO [main: run_main_batch.py, 51] pid:52, epoch:65, args:Namespace(Fold=1, add_copynet=True, add_memory_module=True, add_num_equ_ids=True, always_truncate_tail=True, amp=False, attention_probs_dropout_prob=0.1, beam_size=1, bert_model='bert-base-chinese', config_path=None, dataset='math23k', do_eval=False, do_l2r_training=False, do_lower_case=False, do_train=False, easy_to_hard=False, eval_batch_size=12, ffn_type=0, finetune_decay=False, forbid_duplicate_ngrams=False, forbid_ignore_word='.', fp16=False, fp32_embedding=False, from_scratch=False, gradient_accumulation_steps=1, has_sentence_oracle=False, hidden_dropout_prob=0.1, is_debug=False, is_delete_early_model=False, is_equ_norm=True, is_single_char=False, is_train=True, label_smoothing=0.1, learning_rate=2e-05, length_penalty=0, local_rank=-1, log_dir='./comment/math23k/output/fine_tune/bert_log', loss_scale=0, mask_prob=0.15, mask_prob_eos=0, mask_source_words=False, mask_whole_word=True, max_analogy_len=512, max_len_a=192, max_len_b=64, max_position_embeddings=512, max_pred=64, max_seq_length=256, max_tgt_length=64, memory_train_file='./preprocess/sim_result/sim_question_by_w2v_train_math23k_equNorm_1fold_top5.pkl', memory_valid_file='./preprocess/sim_result/sim_question_by_w2v_valid_math23k_equNorm_1fold_top5.pkl', min_len=None, mode='s2s', model_recover_path='./comment/math23k/output/fine_tune/REAL_math23k_1fold_CP2_ME_EN_top1/model.epoch.bin', need_score_traces=True, new_pos_ids=False, new_segment_ids=False, ngram_size=3, no_cuda=False, not_predict_token=None, num_equ_size=3, num_qkv=0, num_train_epochs=80.0, num_workers=0, optim_recover_path=None, output_dir='./comment/math23k/output/fine_tune/REAL_math23k_1fold_CP2_ME_EN_top1', pos_shift=True, relax_projection=False, s2s_add_segment=False, s2s_share_segment=False, s2s_special_token=False, save_every_epoch=False, seed=42, seg_emb=False, skipgram_prb=0.0, skipgram_size=1, split='test', start_lr_decay_epoch=40, subset=0, tokenized_input=False, topk=1, train_batch_size=6, trunc_seg='a', used_bertAdam=False, warmup_proportion=0.1, weight_decay=0.01)
[Mon, 06 Sep 2021 07:00:56] INFO [main: run_seq2seq_mwp.py, 70] device: cuda n_gpu: 1, distributed training: False, 16-bits training: False
[Mon, 06 Sep 2021 07:00:57] INFO [get_from_cache: file_utils.py, 197] https://s3.amazonaws.com/models.huggingface.co/bert/bert-base-chinese-vocab.txt not found in cache, downloading to /tmp/tmph3mqap0n
[Mon, 06 Sep 2021 07:01:01] INFO [get_from_cache: file_utils.py, 210] copying /tmp/tmph3mqap0n to cache at ..comment/tmp_folder/bert-cased-pretrained-cache/8a0c070123c1f794c42a29c6904beb7c1b8715741e235bee04aca2c7636fc83f.9b42061518a39ca00b8b52059fd2bede8daa613f8a8671500e518a8c29de8c00
[Mon, 06 Sep 2021 07:01:01] INFO [get_from_cache: file_utils.py, 214] creating metadata file for ..comment/tmp_folder/bert-cased-pretrained-cache/8a0c070123c1f794c42a29c6904beb7c1b8715741e235bee04aca2c7636fc83f.9b42061518a39ca00b8b52059fd2bede8daa613f8a8671500e518a8c29de8c00
[Mon, 06 Sep 2021 07:01:01] INFO [get_from_cache: file_utils.py, 220] removing temp file /tmp/tmph3mqap0n
[Mon, 06 Sep 2021 07:01:01] INFO [from_pretrained: tokenization.py, 197] loading vocabulary file https://s3.amazonaws.com/models.huggingface.co/bert/bert-base-chinese-vocab.txt from cache at ..comment/tmp_folder/bert-cased-pretrained-cache/8a0c070123c1f794c42a29c6904beb7c1b8715741e235bee04aca2c7636fc83f.9b42061518a39ca00b8b52059fd2bede8daa613f8a8671500e518a8c29de8c00
[Mon, 06 Sep 2021 07:01:36] INFO [get_from_cache: file_utils.py, 197] https://s3.amazonaws.com/models.huggingface.co/bert/bert-base-chinese.tar.gz not found in cache, downloading to /tmp/tmpnskx7ym1
[Mon, 06 Sep 2021 07:08:06] INFO [get_from_cache: file_utils.py, 210] copying /tmp/tmpnskx7ym1 to cache at ..comment/tmp_folder/bert-cased-pretrained-cache/42d4a64dda3243ffeca7ec268d5544122e67d9d06b971608796b483925716512.02ac7d664cff08d793eb00d6aac1d04368a1322435e5fe0a27c70b0b3a85327f
[Mon, 06 Sep 2021 07:08:10] INFO [get_from_cache: file_utils.py, 214] creating metadata file for ..comment/tmp_folder/bert-cased-pretrained-cache/42d4a64dda3243ffeca7ec268d5544122e67d9d06b971608796b483925716512.02ac7d664cff08d793eb00d6aac1d04368a1322435e5fe0a27c70b0b3a85327f
[Mon, 06 Sep 2021 07:08:10] INFO [get_from_cache: file_utils.py, 220] removing temp file /tmp/tmpnskx7ym1
[Mon, 06 Sep 2021 07:08:10] INFO [from_pretrained: modeling_mwp.py, 737] loading archive file https://s3.amazonaws.com/models.huggingface.co/bert/bert-base-chinese.tar.gz from cache at ..comment/tmp_folder/bert-cased-pretrained-cache/42d4a64dda3243ffeca7ec268d5544122e67d9d06b971608796b483925716512.02ac7d664cff08d793eb00d6aac1d04368a1322435e5fe0a27c70b0b3a85327f
[Mon, 06 Sep 2021 07:08:10] INFO [from_pretrained: modeling_mwp.py, 745] extracting archive file ..comment/tmp_folder/bert-cased-pretrained-cache/42d4a64dda3243ffeca7ec268d5544122e67d9d06b971608796b483925716512.02ac7d664cff08d793eb00d6aac1d04368a1322435e5fe0a27c70b0b3a85327f to temp dir /tmp/tmp0msltyv5
[Mon, 06 Sep 2021 07:08:26] INFO [from_pretrained: modeling_mwp.py, 839] config.type_vocab_size != state_dict[bert.embeddings.token_type_embeddings.weight] (4 != 2)
[Mon, 06 Sep 2021 07:08:27] INFO [from_pretrained: modeling_mwp.py, 1030] Weights of BertForPreTrainingLossMask not initialized from pretrained model: ['bert.embeddings.num_equ_embeddings.weight', 'copyNet.bias', 'copyNet.decoder.weight', 'copyNet.p_gen_linear.weight', 'cls2.predictions.bias', 'cls2.predictions.transform.dense.weight', 'cls2.predictions.transform.dense.bias', 'cls2.predictions.transform.LayerNorm.weight', 'cls2.predictions.transform.LayerNorm.bias', 'cls2.predictions.decoder.weight', 'cls2.seq_relationship.weight', 'cls2.seq_relationship.bias', 'crit_mask_lm_smoothed.one_hot']
[Mon, 06 Sep 2021 07:08:30] INFO [main: run_seq2seq_mwp.py, 218] ***** CUDA.empty_cache() *****
[Mon, 06 Sep 2021 07:08:30] INFO [main: run_seq2seq_mwp.py, 222] ***** Running training *****
[Mon, 06 Sep 2021 07:08:30] INFO [main: run_seq2seq_mwp.py, 223]   Batch size = 6
[Mon, 06 Sep 2021 07:08:30] INFO [main: run_seq2seq_mwp.py, 224]   Num steps = 246960
[Mon, 06 Sep 2021 07:08:30] INFO [main: run_seq2seq_mwp.py, 245] iter_bar:Iter (loss=X.XXX):   0%|                                                               | 0/3087 [00:00<?, ?it/s]
[Mon, 06 Sep 2021 07:08:30] INFO [main: run_seq2seq_mwp.py, 246] len of iter_bar:3087
